{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e018ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance \n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5df3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(file_path):\n",
    "    with open(file_path, 'r') as file: \n",
    "        text = file.read()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3519c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_sentences = [sentence.lower() for sentence in sentences]\n",
    "\n",
    "    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in cleaned_sentences]\n",
    "\n",
    "# Remove stop words and punctuation\n",
    "    tokenized_sentences = [\n",
    "    [word for word in sentence if word.isalnum() and word not in stop_words] for sentence in tokenized_sentences\n",
    "]\n",
    "\n",
    "    return tokenized_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f4a87e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sentence1, sentence2):\n",
    "\n",
    "    all_words = list(set(sentence1 + sentence2)) \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "\n",
    "# Build word frequency vectors for both sentences\n",
    "\n",
    "    for word in sentence1: \n",
    "        vector1[all_words.index(word)] += 1\n",
    "\n",
    "    for word in sentence2: \n",
    "        vector2[all_words.index(word)] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06b9629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_matrix(sentences):\n",
    "\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)): \n",
    "            if i != j:\n",
    "                similarity_matrix[i][j] = sentence_similarity(sentences[i], sentences[j])\n",
    "\n",
    "\n",
    "    return similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "373118eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank_summarize(text, num_sentences): \n",
    "    sentences = preprocess_text(text)\n",
    "    similarity_matrix = build_similarity_matrix(sentences)\n",
    "    sentence_scores = nx.pagerank(nx.from_numpy_array(similarity_matrix))\n",
    "\n",
    "    ranked_sentences = sorted(((sentence_scores[i], sentence) for i, sentence in enumerate(sentences)), reverse=True)\n",
    "    summarized_sentences = ranked_sentences[:num_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8591d5f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "PowerIterationFailedConvergence",
     "evalue": "(PowerIterationFailedConvergence(...), 'power iteration failed to converge within 100 iterations')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPowerIterationFailedConvergence\u001b[0m           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Summarize the text\u001b[39;00m\n\u001b[0;32m      4\u001b[0m num_sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m----> 5\u001b[0m summary \u001b[38;5;241m=\u001b[39m textrank_summarize(text, num_sentences)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Print the summary \u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary:\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36mtextrank_summarize\u001b[1;34m(text, num_sentences)\u001b[0m\n\u001b[0;32m      2\u001b[0m sentences \u001b[38;5;241m=\u001b[39m preprocess_text(text)\n\u001b[0;32m      3\u001b[0m similarity_matrix \u001b[38;5;241m=\u001b[39m build_similarity_matrix(sentences)\n\u001b[1;32m----> 4\u001b[0m sentence_scores \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mpagerank(nx\u001b[38;5;241m.\u001b[39mfrom_numpy_array(similarity_matrix))\n\u001b[0;32m      6\u001b[0m ranked_sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(((sentence_scores[i], sentence) \u001b[38;5;28;01mfor\u001b[39;00m i, sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sentences)), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m summarized_sentences \u001b[38;5;241m=\u001b[39m ranked_sentences[:num_sentences]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\networkx\\classes\\backends.py:148\u001b[0m, in \u001b[0;36m_dispatch.<locals>.wrapper\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m NetworkXNotImplemented(\n\u001b[0;32m    146\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not implemented by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplugin_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m             )\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\networkx\\algorithms\\link_analysis\\pagerank_alg.py:110\u001b[0m, in \u001b[0;36mpagerank\u001b[1;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;129m@nx\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpagerank\u001b[39m(\n\u001b[0;32m     11\u001b[0m     G,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     dangling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m ):\n\u001b[0;32m     20\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the PageRank of the nodes in the graph.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    PageRank computes a ranking of the nodes in the graph G based on\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m \n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pagerank_scipy(\n\u001b[0;32m    111\u001b[0m         G, alpha, personalization, max_iter, tol, nstart, weight, dangling\n\u001b[0;32m    112\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\networkx\\algorithms\\link_analysis\\pagerank_alg.py:500\u001b[0m, in \u001b[0;36m_pagerank_scipy\u001b[1;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m<\u001b[39m N \u001b[38;5;241m*\u001b[39m tol:\n\u001b[0;32m    499\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(nodelist, \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mfloat\u001b[39m, x)))\n\u001b[1;32m--> 500\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mPowerIterationFailedConvergence(max_iter)\n",
      "\u001b[1;31mPowerIterationFailedConvergence\u001b[0m: (PowerIterationFailedConvergence(...), 'power iteration failed to converge within 100 iterations')"
     ]
    }
   ],
   "source": [
    "file_path = 'sample_txt.txt' \n",
    "text = read_text(file_path)\n",
    "# Summarize the text\n",
    "num_sentences = 3\n",
    "summary = textrank_summarize(text, num_sentences)\n",
    "\n",
    "\n",
    "# Print the summary \n",
    "print(\"Summary:\") \n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f000a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Obtaining dependency information for pyyaml>=5.1 from https://files.pythonhosted.org/packages/b3/34/65bb4b2d7908044963ebf614fe0fdb080773fc7030d7e39c8d3eddcd4257/PyYAML-6.0.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Downloading PyYAML-6.0.1-cp311-cp311-win_amd64.whl (144 kB)\n",
      "   ---------------------------------------- 0.0/144.7 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/144.7 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/144.7 kB ? eta -:--:--\n",
      "   -------- ------------------------------ 30.7/144.7 kB 435.7 kB/s eta 0:00:01\n",
      "   ------------------- ------------------- 71.7/144.7 kB 491.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- 144.7/144.7 kB 781.7 kB/s eta 0:00:00\n",
      "Installing collected packages: pyyaml\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed pyyaml-6.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\n",
      "chatterbot-corpus 1.2.0 requires PyYAML<4.0,>=3.12, but you have pyyaml 6.0.1 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires python-dateutil==2.8.2, but you have python-dateutil 2.7.5 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba05411f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-small and revision d769bba (https://huggingface.co/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e614a7a7eaf4e0cae6f0bec8eee120a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Hp\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9eeb1e78154516a02c061390acfd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a167dc7b584fc39072f2628055c81d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b042169fed4921a99d1c47b32f8080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24438502ca804361927a03eb84684f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'YouTube is a popular video-sharing website'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a summarization pipeline\n",
    "summarizer = pipeline('summarization')\n",
    "\n",
    "# Define the article text\n",
    "article = \"\"\"YouTube is a popular video-sharing website created in 2005. It allows users to upload, share, and view videos, as well as subscribe to channels of their interests. YouTube has a strong community of content creators, and it is a great platform for watching videos on a wide variety of topics, including music, tutorials, gaming, comedy, education, and more. The website has become a beloved pastime for millions of users around the world.\"\"\"\n",
    "\n",
    "# Summarize the article\n",
    "summary = summarizer(article, max_length=10, min_length=5, do_sample=False)\n",
    "\n",
    "# Print the summarized text\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0b5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
